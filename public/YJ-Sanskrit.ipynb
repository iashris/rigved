{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c260bd-21d8-4e40-9a7f-509a01e78735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yajurveda (TITUS) — Devanagari scraper with simple keys\n",
    "# Outputs:\n",
    "#   devanagari_black.json  # {\"x.y.z\": \"देवना…\", ...}\n",
    "#   devanagari_white.json  # {\"a.b\":   \"देवना…\", ...}\n",
    "#\n",
    "# Notes:\n",
    "# - BLACK source:  http://titus.uni-frankfurt.de/texte/etcd/ind/aind/ved/yvs/ts/ts001.htm ...\n",
    "#   key = Book.Chapter.Verse (x.y.z)  (Devanagari sentences joined)\n",
    "# - WHITE source: http://titus.uni-frankfurt.de/texte/etcd/ind/aind/ved/yvw/vs/vs001.htm ...\n",
    "#   key = Paragraph.Verse (a.b)       (Devanagari sentences joined)\n",
    "\n",
    "import os, re, json, time, html as htmlmod\n",
    "import requests\n",
    "\n",
    "# ----------------------------\n",
    "# HTTP fetch with sane headers\n",
    "# ----------------------------\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/124 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def fetch(url: str) -> str:\n",
    "    r = SESSION.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = \"utf-8\"\n",
    "    return r.text\n",
    "\n",
    "# --------------------------------------\n",
    "# HTML → plain text normalization (TITUS)\n",
    "# --------------------------------------\n",
    "def titus_html_to_text(html: str) -> str:\n",
    "    s = html\n",
    "    # normalize all <br> flavors / </p> to newlines\n",
    "    s = re.sub(r'(?is)<\\s*br\\s*/?\\s*>', '\\n', s)\n",
    "    s = re.sub(r'(?is)</\\s*p\\s*>', '\\n', s)\n",
    "    # drop tags\n",
    "    s = re.sub(r'(?is)<[^>]+>', '', s)\n",
    "    # decode entities (&nbsp; etc.)\n",
    "    s = htmlmod.unescape(s)\n",
    "    # normalize odd spaces\n",
    "    s = s.replace('\\xa0', ' ').replace('\\u2009', ' ')\n",
    "    s = s.replace('\\r', '')\n",
    "    # collapse and trim lines\n",
    "    lines = [re.sub(r'\\s+', ' ', ln).strip() for ln in s.split('\\n')]\n",
    "    lines = [ln for ln in lines if ln]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# Tolerant label regexes\n",
    "RE_BOOK  = re.compile(r'^\\s*Book:\\s*(\\d+)\\s*$', re.I)\n",
    "RE_CHAP  = re.compile(r'^\\s*Chapter:\\s*(\\d+)\\s*$', re.I)\n",
    "RE_PARA  = re.compile(r'^\\s*Paragraph:\\s*(\\d+)\\s*$', re.I)   # = Adhyāya for White\n",
    "RE_VERSE = re.compile(r'^\\s*Verse:\\s*(\\d+)\\s*$', re.I)       # = Mantra\n",
    "RE_SENT  = re.compile(r'^\\s*Sentence:\\s*([A-Za-z0-9=]+)\\s+(.*)$', re.I)\n",
    "\n",
    "def parse_titus_page(html: str):\n",
    "    \"\"\"\n",
    "    Extract records in appearance order. Works for both VS (white) and TS (black).\n",
    "    Emits dicts with any of: book, chapter, paragraph, verse, sentence, text.\n",
    "    \"\"\"\n",
    "    text = titus_html_to_text(html)\n",
    "    recs = []\n",
    "    book = chapter = para = verse = None\n",
    "    for ln in text.split('\\n'):\n",
    "        m = RE_BOOK.match(ln)\n",
    "        if m: book = int(m.group(1));   continue\n",
    "        m = RE_CHAP.match(ln)\n",
    "        if m: chapter = int(m.group(1)); continue\n",
    "        m = RE_PARA.match(ln)\n",
    "        if m: para = int(m.group(1));    continue\n",
    "        m = RE_VERSE.match(ln)\n",
    "        if m: verse = int(m.group(1));   continue\n",
    "        m = RE_SENT.match(ln)\n",
    "        if m:\n",
    "            recs.append({\n",
    "                \"book\": book, \"chapter\": chapter,\n",
    "                \"paragraph\": para, \"verse\": verse,\n",
    "                \"sentence\": m.group(1),\n",
    "                \"text\": m.group(2)\n",
    "            })\n",
    "    return recs\n",
    "\n",
    "def iter_parts(base_url: str, stem: str, start=1, max_holes=3, sleep=0.25):\n",
    "    \"\"\"\n",
    "    Iterate sequential part pages: base/stem001.htm, stem002.htm, ...\n",
    "    Stop after `max_holes` consecutive failures (end of series).\n",
    "    \"\"\"\n",
    "    holes = 0\n",
    "    i = start\n",
    "    while True:\n",
    "        url = f\"{base_url}/{stem}{i:03d}.htm\"\n",
    "        try:\n",
    "            html = fetch(url)\n",
    "            yield i, url, html\n",
    "            holes = 0\n",
    "        except Exception:\n",
    "            holes += 1\n",
    "            if holes >= max_holes:\n",
    "                break\n",
    "        i += 1\n",
    "        time.sleep(sleep)\n",
    "\n",
    "def group_join(records, key_fields):\n",
    "    \"\"\"\n",
    "    Join all Sentence texts under the given key tuple -> single string.\n",
    "    \"\"\"\n",
    "    buckets = {}\n",
    "    for r in records:\n",
    "        if any(r.get(k) is None for k in key_fields):\n",
    "            continue\n",
    "        k = '.'.join(str(r[k]) for k in key_fields)\n",
    "        buckets.setdefault(k, []).append(r[\"text\"])\n",
    "    return {k: ' '.join(v) for k, v in buckets.items()}\n",
    "\n",
    "# -----------------------\n",
    "# WHITE: a.b (adhyaya.mantra)\n",
    "# -----------------------\n",
    "def scrape_white_to_json(out_path=\"devanagari_white.json\"):\n",
    "    base = \"http://titus.uni-frankfurt.de/texte/etcd/ind/aind/ved/yvw/vs\"\n",
    "    allrecs = []\n",
    "    for idx, url, html in iter_parts(base, \"vs\", start=1, max_holes=3):\n",
    "        recs = parse_titus_page(html)\n",
    "        allrecs.extend(recs)\n",
    "    # Paragraph=Adhyaya (a), Verse=Mantra (b)\n",
    "    rolled = group_join(allrecs, (\"paragraph\", \"verse\"))\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rolled, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[WHITE] parts={idx} sentences={len(allrecs)} unique a.b={len(rolled)} → {out_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# BLACK: x.y.z (book.chapter.verse)\n",
    "# -----------------------\n",
    "def scrape_black_to_json(out_path=\"devanagari_black.json\"):\n",
    "    base = \"http://titus.uni-frankfurt.de/texte/etcd/ind/aind/ved/yvs/ts\"\n",
    "    allrecs = []\n",
    "    for idx, url, html in iter_parts(base, \"ts\", start=1, max_holes=3):\n",
    "        recs = parse_titus_page(html)\n",
    "        allrecs.extend(recs)\n",
    "    # Book=Kāṇḍa (x), Chapter=Prapāṭhaka (y), Verse (z)\n",
    "    rolled = group_join(allrecs, (\"book\", \"chapter\", \"verse\"))\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rolled, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[BLACK] parts={idx} sentences={len(allrecs)} unique x.y.z={len(rolled)} → {out_path}\")\n",
    "\n",
    "# run both\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c021c10-39cb-4cfc-bafa-d53a27ac2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLACK] parts=46 sentences=16405 unique x.y.z=54 → devanagari_black.json\n"
     ]
    }
   ],
   "source": [
    "# scrape_white_to_json(\"devanagari_white.json\")\n",
    "scrape_black_to_json(\"devanagari_black.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1e8e0-2d44-468f-9a15-d3652b9e0e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
